# -*- coding: utf-8 -*-
"""Weather_Pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AASAOxN-VOBvh3SnwjMqdXILB6lSE-tb

#Data loading
"""

!pip install opendatasets --upgrade --quiet

import opendatasets as od
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

# Download the dataset
od.download('https://www.kaggle.com/jsphyg/weather-dataset-rattle-package')
raw_df = pd.read_csv('weather-dataset-rattle-package/weatherAUS.csv')
raw_df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)

raw_df.head()

raw_df.info()

"""#Data Visualization"""

# Commented out IPython magic to ensure Python compatibility.
#Importing libraries
import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#setup plotting parameters
sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10, 6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

px.histogram(raw_df, x='Location', title='Location vs. Rainy Days', color='RainToday')

"""Insight:

*  We have mostly uniform distribution of data across locations.
*  Seems to be roughly the cases that about 20% of days in pretty much every city there was rain.
*   And then for about 80 or 75% of days, there was no rain.
*   There is some correlation between Location and Rain.

"""

px.histogram(raw_df,
             x='Temp3pm',
             title='Temperature at 3 pm vs. Rain Tomorrow',
             color='RainTomorrow')

"""Insight:


*  The red indicates the cases where it did rain tomorrow and blue indicates no rain.
*  So, if the temperature at 3pm is lower, it's more,it seems likely that it would rain tomorrow.

*  Although, there are still a decent number of examples where 3pm temperature has been high and it has still rained tomorrow.
"""

px.histogram(raw_df,
             x='RainTomorrow',
             color='RainToday',
             title='Rain Tomorrow vs. Rain Today')

"""Insight:


*   The proportion of rain tomorrow is very low as compared to no rain .
*   If it did not rain today, then there's a pretty good chance that it will not rain tomorrow,so predicting rain tomorrow 'Yes' is difficult and predicting rain tomorrow 'No' is easier
*   And there is class imbalance.





"""

px.scatter(raw_df.sample(2000),
           title='Min Temp. vs Max Temp.',
           x='MinTemp',
           y='MaxTemp',
           color='RainToday')

"""Insight:


*  There is a linear correlation between MinTemp & MaxTemp.
*  When it rains on a particular day, then the variation in temperature is small.
*  And maybe that is also relevant for whether it rains tomorrow or not.




"""

px.scatter(raw_df.sample(2000),
           title='Temp (3 pm) vs. Humidity (3 pm)',
           x='Temp3pm',
           y='Humidity3pm',
           color='RainTomorrow')

"""Insight:


* All the red points are in the low temperature and high humidity region,
And blue points are in the high temperature and low humidity region.

*  If the temperature today's low and humidity is high, then there's a high chance that there maybe rain tomorrow.


"""

plt.title('No. of Rows per Year')
sns.countplot(x=pd.to_datetime(raw_df.Date).dt.year);

"""Insight:


*   Mostly, there's normal distribution of data in years.
*   Most of the data are lies between 2009 - 2016.

#Data Preprocessing
"""

# Create training, validation and test sets
year = pd.to_datetime(raw_df.Date).dt.year

train_df = raw_df[year < 2015]
val_df = raw_df[year == 2015]
test_df = raw_df[year > 2015]

print('train_df.shape :', train_df.shape)
print('val_df.shape :', val_df.shape)
print('test_df.shape :', test_df.shape)

"""Identifying Input and Target Columns
Often, not all the columns in a dataset are useful for training a model. In the current dataset, we can ignore the Date column, since we only want to weather conditions to make a prediction about whether it will rain the next day.

Let's create a list of input columns, and also identify the target column.

# Create inputs and targets
"""

# Create inputs and targets
input_cols = list(train_df.columns)[1:-1]
target_col = 'RainTomorrow'

print(input_cols)

target_col

train_inputs = train_df[input_cols].copy()
train_targets = train_df[target_col].copy()

val_inputs = val_df[input_cols].copy()
val_targets = val_df[target_col].copy()

test_inputs = test_df[input_cols].copy()
test_targets = test_df[target_col].copy()

"""Let's also identify which of the columns are numerical and which ones are categorical. This will be useful later, as we'll need to convert the categorical data to numbers for training a logistic regression model.

# Identify numeric and categorical columns
"""

numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()
categorical_cols = train_inputs.select_dtypes('object').columns.tolist()

"""Do the ranges of the numeric columns seem reasonable? If not, we may have to do some data cleaning as well.

Imputing Missing Numeric Data
Machine learning models can't work with missing numerical data. The process of filling missing values is called imputation.

The first step in imputation is to `fit` the imputer to the data i.e. compute the chosen statistic (e.g. mean) for each column in the dataset.
"""

imputer = SimpleImputer(strategy = 'mean')

imputer.fit(raw_df[numeric_cols])

"""After calling `fit`, the computed statistic for each column is stored in the `statistics_` property of `imputer`."""

list(imputer.statistics_)

"""The missing values in the training, test and validation sets can now be filled in using the `transform` method of `imputer`."""

train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])
val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])
test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])

"""The missing values are now filled in with the mean of each column.

Scaling Numeric Features
Another good practice is to scale numeric features to a small range of values e.g.
(
0
,
1
)
(0,1) or
(
−
1
,
1
)
(−1,1). Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.

Let's use `MinMaxScaler` from `sklearn.preprocessing` to scale values to the $(0,1)$ range.
"""

scaler = MinMaxScaler()

scaler.fit(raw_df[numeric_cols])

"""We can now inspect the minimum and maximum values in each column."""

print('Minimum:')
list(scaler.data_min_)

print('Maximum:')
list(scaler.data_max_)

"""We can now separately scale the training, validation and test sets using the `transform` method of `scaler`."""

train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])
val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])
test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])

"""Encoding Categorical Data
Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.
"""

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

encoder.fit(raw_df[categorical_cols])

encoder.categories_

"""The encoder has created a list of categories for each of the categorical columns in the dataset.

We can generate column names for each individual category using `get_feature_names_out
`.
"""

encoded_cols = list(encoder.get_feature_names_out(categorical_cols))
print(encoded_cols)

"""All of the above columns will be added to `train_inputs`, `val_inputs` and `test_inputs`.

To perform the encoding, we use the `transform` method of `encoder`.
"""

train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])
val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])
test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])

# To set an No limit in displaying columns
pd.set_option('display.max_columns', None)

test_inputs

"""# Save processed data to disk"""

!pip install pyarrow --quiet

train_inputs.to_parquet('train_inputs.parquet')
val_inputs.to_parquet('val_inputs.parquet')
test_inputs.to_parquet('test_inputs.parquet')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# pd.DataFrame(train_targets).to_parquet('train_targets.parquet')
# pd.DataFrame(val_targets).to_parquet('val_targets.parquet')
# pd.DataFrame(test_targets).to_parquet('test_targets.parquet')

"""We can read the data back using `pd.read_parquet`."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# train_inputs = pd.read_parquet('train_inputs.parquet')
# val_inputs = pd.read_parquet('val_inputs.parquet')
# test_inputs = pd.read_parquet('test_inputs.parquet')
# 
# train_targets = pd.read_parquet('train_targets.parquet')[target_col]
# val_targets = pd.read_parquet('val_targets.parquet')[target_col]
# test_targets = pd.read_parquet('test_targets.parquet')[target_col]

"""Let's verify that the data was loaded properly."""

print('train_inputs:', train_inputs.shape)
print('train_targets:', train_targets.shape)
print('val_inputs:', val_inputs.shape)
print('val_targets:', val_targets.shape)
print('test_inputs:', test_inputs.shape)
print('test_targets:', test_targets.shape)

"""# Model Training and Evaluation

Training a Logistic Regression Model
Logistic regression is a commonly used technique for solving binary classification problems. In a logistic regression model:
"""

# Import the LogisticRegression model
from sklearn.linear_model import LogisticRegression

# Create and train the model
model = LogisticRegression(solver='liblinear')

"""We can train the model using `model.fit`."""

model.fit(train_inputs[numeric_cols + encoded_cols], train_targets)

#To show the features and their weights & intercept
print(numeric_cols + encoded_cols)

print(model.coef_.tolist())

print(model.intercept_)

"""We can create a DataFrame of features and their weights, then we can conclude the top features, those have more impact on target column

Each weight is applied to the value in a specific column of the input. Higher the weight, greater the impact of the column on the prediction.
"""

n = len(model.coef_.tolist())

weight_df=pd.DataFrame({
    'feature': numeric_cols + encoded_cols,
    'weight': model.coef_.tolist()[0]
})

# Visualize the features with their weights
import seaborn as sns
sns.barplot(data=weight_df.sort_values('weight',ascending=False).head(10),x='weight',y='feature')

"""Making Predictions and Evaluating the Model
We can now use the trained model to make predictions on the training, test
"""

# Select the columns to be used for training/prediction
X_train = train_inputs[numeric_cols + encoded_cols]
X_val = val_inputs[numeric_cols + encoded_cols]
X_test = test_inputs[numeric_cols + encoded_cols]

""" Generate predictions and probabilities"""

train_preds = model.predict(X_train)

"""We can output a probabilistic prediction using `predict_proba`."""

train_probs = model.predict_proba(X_train)
train_probs

"""The numbers above indicate the probabilities for the target classes "No" and "Yes"."""

model.classes_

"""We can test the accuracy of the model's predictions by computing the percentage of matching values in `train_preds` and `train_targets`.

This can be done using the `accuracy_score` function from `sklearn.metrics`.
"""

#Importing the classification_report and accuracy metrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

print(classification_report(train_targets, train_preds))

"""The model achieves an accuracy of 85.1% on the training set. We can visualize the breakdown of correctly and incorrectly classified inputs using a confusion matrix.

<img src="https://i.imgur.com/UM28BCN.png" width="480">

Depending on the business usecases or what you want to use the model for,you may want to very closely look at false positives or false negatives.for example, if you use this model to predict whether a baseball game should be hosted tomorrow in the open, then false negatives are a big problem.
you may want to avoid false negatives where it is going to rain tomorrow, but your model predicts that is  not going to rain, so if you have a high number of false negatives,that means you are in for a surprise where you get a 50,000 people and people are playing baseball and starts rain.
That's why you maybe want you to optimize. you want to reduce the false negatives and you are  not just interested in accuracy.

On the other hand,let's say it's a different kind of model,which is predicting whether you may have breast cancer. Then at that point, you may want to take a closer look at False Positive,before let's say recommending, I don't know , chemotherapy to somebody, you want to make sure that your model,you want to make sure that it is not a false positive and for that while training the model,you want to make sure that the number of false positive low

So in general,depending on the kind of problem you may want to optimize either for False positives,all for false negatives
"""

#Importing confusion metrics
from sklearn.metrics import confusion_matrix

confusion_matrix(train_targets, train_preds, normalize='true')

"""So based on the confusion matrix,if the model says that it's not going to rain tomorrow, I'm probably not that sure,because there is still 47% chances that the model says that is not going to rain tomorrow

Let's define a helper function to generate predictions, compute the accuracy score and plot a confusion matrix for a given st of inputs.
"""

def predict_and_plot(inputs,targets, name=''):
  preds = model.predict(inputs)

  accuracy = accuracy_score(targets,preds)
  print('Accuracy: {:2f}%'.format(accuracy * 100))

  cf = confusion_matrix(targets, preds, normalize='true')
  plt.figure()
  sns.heatmap(cf, annot = True)
  plt.xlabel('Prediction')
  plt.ylabel('Target')
  plt.title('{} Confusion_Matrix'.format(name));

  return preds

"""Evaluate on train set"""

train_preds = predict_and_plot(X_train, train_targets, 'Training')

"""Let's compute the model's accuracy on the validation and test sets too."""

val_preds = predict_and_plot(X_val, val_targets, 'Validation')

test_preds = predict_and_plot(X_test, test_targets, 'Test')

"""The accuracy of the model on the test and validation set are above 84%, which suggests that our model generalizes well to data it hasn't seen before.

But how good is 84% accuracy? While this depends on the nature of the problem and on business requirements, a good way to verify whether a model has actually learned something useful is to compare its results to a "random" or "dumb" model.

Let's create two models: one that guesses randomly and another that always return "No". Both of these models completely ignore the inputs given to them.
"""

def random_guess(inputs):
  return np.random.choice(['No','Yes'],len(inputs))

def all_no(inputs):
  return np.full(len(inputs),'No')

"""Let's check the accuracies of these two models on the test set."""

accuracy_score(test_targets, random_guess(X_test))

accuracy_score(test_targets, all_no(X_test))

"""Our random model achieves an accuracy of 50% and our "always No" model achieves an accuracy of 77%.

Thankfully, our model is better than a "dumb" or "random" model! This is not always the case, so it's a good practice to benchmark any model you train against such baseline models.

## Making Predictions on a Single Input

Once the model has been trained to a satisfactory accuracy, it can be used to make predictions on new data. Consider the following dictionary containing data collected from the Katherine weather department today.
"""

new_input = {'Date': '2021-06-19',
             'Location': 'Katherine',
             'MinTemp': 23.2,
             'MaxTemp': 33.2,
             'Rainfall': 10.2,
             'Evaporation': 4.2,
             'Sunshine': np.nan,
             'WindGustDir': 'NNW',
             'WindGustSpeed': 15.0,
             'WindDir9am': 'NW',
             'WindDir3pm': 'NNE',
             'WindSpeed9am': 13.0,
             'WindSpeed3pm': 20.0,
             'Humidity9am': 89.0,
             'Humidity3pm': 58.0,
             'Pressure9am': 1004.8,
             'Pressure3pm': 1001.5,
             'Cloud9am': 8.0,
             'Cloud3pm': 5.0,
             'Temp9am': 25.7,
             'Temp3pm': 33.0,
             'RainToday': 'Yes'}

"""The first step is to convert the dictionary into a Pandas dataframe, similar to `raw_df`. This can be done by passing a list containing the given dictionary to the `pd.DataFrame` constructor."""

new_input_df = pd.DataFrame([new_input])
new_input_df

"""We must now apply the same transformations applied while training the model:

Imputation of missing values using the imputer created earlier.

Scaling numerical features using the scaler created earlier.

Encoding categorical features using the encoder created earlier.
"""

new_input_df[numeric_cols] = imputer.transform(new_input_df[numeric_cols])
new_input_df[numeric_cols] = scaler.transform(new_input_df[numeric_cols])
new_input_df[encoded_cols] = encoder.transform(new_input_df[categorical_cols])

X_new_input = new_input_df[numeric_cols + encoded_cols]
X_new_input

"""We can now make a prediction using model.predict.

"""

prediction = model.predict(X_new_input)[0]

prediction

"""Our model predicts that it will rain tomorrow in Katherine! We can also check the probability of the prediction."""

prob = model.predict_proba(X_new_input)[0]

prob

"""## Saving and Loading Trained Models

We can save the parameters (weights and biases) of our trained model to disk, so that we needn't retrain the model from scratch each time we wish to use it. Along with the model, it's also important to save imputers, scalers, encoders and even column names. Anything that will be required while generating predictions using the model should be saved.

We can use the `joblib` module to save and load Python objects on the disk.
"""

import joblib

aussie_rain = {
    'model': model,
    'imputer': imputer,
    'scaler': scaler,
    'encoder': encoder,
    'input_cols': input_cols,
    'target_col': target_col,
    'numeric_cols': numeric_cols,
    'categorical_cols': categorical_cols,
    'encoded_cols': encoded_cols
}

"""We can now save this to a file using `joblib.dump`"""

joblib.dump(aussie_rain, 'aussie_rain.joblib')

"""The object can be loaded back using `joblib.load`"""

aussie_rain2 = joblib.load('aussie_rain.joblib')

"""Let's use the loaded model to make predictions on the original test set."""

test_preds2 = aussie_rain2['model'].predict(X_test)
accuracy_score(test_targets, test_preds2)

"""As expected, we get the same result as the original model.

## Summary and References

Logistic regression is a commonly used technique for solving binary classification problems. In a logistic regression model:

- we take linear combination (or weighted sum of the input features)
- we apply the sigmoid function to the result to obtain a number between 0 and 1
- this number represents the probability of the input being classified as "Yes"
- instead of RMSE, the cross entropy loss function is used to evaluate the results


<img src="https://i.imgur.com/YMaMo5D.png" width="480">


To train a logistic regression model, we can use the `LogisticRegression` class from Scikit-learn. We covered the following topics in this tutorial:

- Downloading a real-world dataset from Kaggle
- Exploratory data analysis and visualization
- Splitting a dataset into training, validation & test sets
- Filling/imputing missing values in numeric columns
- Scaling numeric features to a $(0,1)$ range
- Encoding categorical columns as one-hot vectors
- Training a logistic regression model using Scikit-learn
- Evaluating a model using a validation set and test set
- Saving a model to disk and loading it back
"""

